from ray import serve
import random, asyncio

RESPONSES = [
    "Hello! I'm your fake LLM ü§ñ",
    "I totally understood your question (trust me üòÖ).",
    "The answer is... 42.",
    "I'm only pretending to be smart, but it works!",
    "Yes! Scaling is working üöÄ"
]

# M·ªói replica 1 CPU => √©p scale ra nhi·ªÅu worker pod khi v∆∞·ª£t s·ª©c 1 pod
@serve.deployment(ray_actor_options={"num_cpus": 1})
class FakeLLM:
    async def __call__(self, request):
        data = await request.json()
        prompt = data.get("prompt", "")
        await asyncio.sleep(0.5)
        return {"response": f"Q: {prompt}\nA: {random.choice(RESPONSES)}"}

deployment_graph = FakeLLM.bind()
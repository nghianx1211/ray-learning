# Tensor Parallelism vs Concurrency

## âš ï¸ Quan trá»ng: Tensor Parallelism yÃªu cáº§u NHIá»€U GPUs

### Tensor Parallelism lÃ  gÃ¬?

**Tensor Parallelism (TP)** lÃ  ká»¹ thuáº­t chia nhá» model weights ra nhiá»u GPUs Ä‘á»ƒ:
- Load models lá»›n hÆ¡n (vÆ°á»£t quÃ¡ memory cá»§a 1 GPU)
- TÄƒng tá»‘c inference báº±ng cÃ¡ch tÃ­nh toÃ¡n song song trÃªn nhiá»u GPUs

### YÃªu cáº§u resources cho Tensor Parallelism:

```yaml
tensor_parallel_size: 1  # YÃªu cáº§u: 1 GPU
tensor_parallel_size: 2  # YÃªu cáº§u: 2 GPUs
tensor_parallel_size: 4  # YÃªu cáº§u: 4 GPUs
```

**CHÃš Ã:** vLLM vá»›i `tensor_parallel_size: 1` váº«n yÃªu cáº§u **2 GPUs** vÃ¬:
1. 1 GPU cho model
2. 1 GPU cho placement group (Ray internal)

### âŒ Lá»—i khi dÃ¹ng TP trÃªn single GPU:

```
Error: No available node types can fulfill resource request 
{'CPU': 2.0, 'GPU': 2.0}
```

Lá»—i nÃ y xuáº¥t hiá»‡n khi:
- Set `tensor_parallel_size: 1`
- NhÆ°ng cluster chá»‰ cÃ³ 1 GPU
- vLLM yÃªu cáº§u 2 GPUs â†’ **khÃ´ng Ä‘á»§ resources**

---

## âœ… Concurrency KHÃ”NG cáº§n nhiá»u GPUs

### Concurrency lÃ  gÃ¬?

**Concurrency** (Ä‘á»“ng thá»i) lÃ  kháº£ nÄƒng xá»­ lÃ½ nhiá»u requests cÃ¹ng lÃºc trÃªn **CÃ™NG 1 replica**.

### CÃ¡ch tÄƒng concurrency trÃªn single GPU:

```yaml
deployment_config:
  ray_actor_options:
    num_cpus: 2  # TÄƒng CPU Ä‘á»ƒ handle nhiá»u requests
    num_gpus: 0.95  # Váº«n chá»‰ dÃ¹ng 1 GPU
  max_ongoing_requests: 256  # Cho phÃ©p 256 requests Ä‘á»“ng thá»i

engine_kwargs:
  tensor_parallel_size: 0  # KHÃ”NG dÃ¹ng TP
  max_num_seqs: 256  # Batch nhiá»u requests láº¡i
  max_num_batched_tokens: 32768  # TÄƒng batch size
  enforce_eager: false  # DÃ¹ng CUDA graphs â†’ faster
```

### Lá»£i Ã­ch:

âœ… **KhÃ´ng cáº§n thÃªm GPU**
âœ… **TÄƒng throughput** (requests/second)
âœ… **Giáº£m latency** nhá» batching
âœ… **Tá»‘i Æ°u GPU utilization**

---

## ğŸ“Š So sÃ¡nh

| Feature | Tensor Parallelism | Concurrency |
|---------|-------------------|-------------|
| **YÃªu cáº§u GPUs** | 2+ GPUs | 1 GPU |
| **Má»¥c Ä‘Ã­ch** | Load large models | Handle nhiá»u requests |
| **TÄƒng throughput** | CÃ³ (náº¿u cÃ³ nhiá»u GPUs) | CÃ³ (trÃªn 1 GPU) |
| **Setup** | Phá»©c táº¡p | ÄÆ¡n giáº£n |
| **Single GPU node** | âŒ KhÃ´ng dÃ¹ng Ä‘Æ°á»£c | âœ… DÃ¹ng Ä‘Æ°á»£c |

---

## ğŸ¯ Khuyáº¿n nghá»‹ cho single-GPU node

### Config tá»‘i Æ°u:

```yaml
applications:
  - name: "falcon3-1b-instruct-app"
    import_path: "ray.serve.llm:build_openai_app"
    route_prefix: "/v1"
    args:
      llm_configs:
        - model_loading_config:
            model_id: "falcon3-1b-instruct"
            model_source: "tiiuae/Falcon3-1B-Instruct"
          deployment_config:
            name: "falcon3-1b-deployment"
            ray_actor_options:
              num_cpus: 2  # TÄƒng CPU cho concurrency
              num_gpus: 0.95  # Single GPU
            autoscaling_config:
              min_replicas: 1
              max_replicas: 1  # Single replica trÃªn 1 GPU
            max_ongoing_requests: 256  # High concurrency
          engine_kwargs:
            tensor_parallel_size: 0  # KHÃ”NG dÃ¹ng TP
            dtype: "float16"
            gpu_memory_utilization: 0.90
            max_model_len: 8192
            enforce_eager: false  # CUDA graphs
            max_num_seqs: 256  # Batch size
            max_num_batched_tokens: 32768
            trust_remote_code: true
```

### Káº¿t quáº£ mong Ä‘á»£i:

- âœ… Deploy thÃ nh cÃ´ng trÃªn 1 GPU
- âœ… Handle 256 concurrent requests
- âœ… High throughput nhá» batching
- âœ… Low latency nhá» CUDA graphs
- âœ… GPU utilization ~90%

---

## ğŸ”§ Khi nÃ o nÃªn dÃ¹ng Tensor Parallelism?

Chá»‰ dÃ¹ng TP khi:

1. **Model quÃ¡ lá»›n** (khÃ´ng fit vÃ o 1 GPU)
   - VÃ­ dá»¥: Llama 70B, Falcon 180B
2. **CÃ³ nhiá»u GPUs** (2+)
   - VÃ­ dá»¥: 2Ã— A100, 4Ã— V100
3. **Cáº§n extreme throughput**
   - Serving production vá»›i high traffic

Vá»›i **Falcon 1B-3B models** trÃªn **single GPU**:
- âŒ KHÃ”NG cáº§n TP
- âœ… TÄƒng concurrency thay vÃ¬

---

## ğŸ“ Testing

### Test concurrency:

```bash
# Deploy vá»›i concurrency config
python3 app_v5.py single_model_optimized.yaml

# Test vá»›i nhiá»u requests Ä‘á»“ng thá»i
for i in {1..10}; do
  curl -X POST http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
      "model": "falcon3-1b-instruct",
      "messages": [{"role": "user", "content": "Hello '$i'"}],
      "temperature": 0.7
    }' &
done
wait

# Check throughput
echo "All 10 requests completed"
```

### Monitor GPU:

```bash
watch -n 1 nvidia-smi
```

Báº¡n sáº½ tháº¥y:
- GPU utilization ~90%
- Memory usage stable
- Multiple requests Ä‘Æ°á»£c batch láº¡i

---

## ğŸš€ TÃ³m táº¯t

**Single GPU node:**
- âœ… Use `tensor_parallel_size: 0`
- âœ… Increase `max_ongoing_requests`
- âœ… Increase `max_num_seqs`
- âœ… Use `enforce_eager: false`
- âŒ DO NOT use `tensor_parallel_size: 1+`

**Multi-GPU node:**
- âœ… Use `tensor_parallel_size: <num_gpus>`
- âœ… Increase `num_gpus` in `ray_actor_options`
- âœ… Adjust `gpu_memory_utilization` accordingly


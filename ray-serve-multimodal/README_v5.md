# Ray Serve Multi-Model Deployment V5 (app_v5.py)

## ğŸ¯ Cáº£i tiáº¿n so vá»›i V4

### 1. **Concurrent Deployment (Deploy Ä‘á»“ng thá»i)**
- âœ… Deploy nhiá»u models cÃ¹ng lÃºc thay vÃ¬ tuáº§n tá»±
- âœ… Nhanh hÆ¡n Ä‘Ã¡ng ká»ƒ khi cÃ³ nhiá»u models
- âœ… KhÃ´ng bá»‹ block khi má»™t model load lÃ¢u

### 2. **Per-Replica Resource Allocation (TÃ i nguyÃªn chÃ­nh xÃ¡c)**
- âœ… Má»—i replica chá»‰ dÃ¹ng Ä‘Ãºng sá»‘ CPUs/GPUs Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh
- âœ… KhÃ´ng chiáº¿m full node resources
- âœ… Tá»± Ä‘á»™ng map `num_cpus/cpus` vÃ  `num_gpus/gpus` vÃ o `ray_actor_options`

## ğŸš€ CÃ¡ch sá»­ dá»¥ng

### Cháº¡y cÆ¡ báº£n:
```bash
python3 app_v5.py multi_model_config.yaml
```

### Output máº«u:
```
ğŸ”§ Initializing Ray and Ray Serve...
ğŸ“– Loading config from: multi_model_config.yaml
ğŸ“¦ Found 2 application(s) to deploy
ğŸ”„ Starting concurrent deployment...

ğŸš€ Deploying: falcon3-1b-instruct-app
   ğŸ“¦ Model ID: falcon3-1b-instruct
   ğŸŒ Route: /falcon3-1b
   ğŸ“Š Mapped resources to ray_actor_options: {'num_cpus': 1.0, 'num_gpus': 0.45}
   âœ… Deployed successfully: falcon3-1b-instruct-app

ğŸš€ Deploying: falcon-h1-0.5b-instruct-app
   ğŸ“¦ Model ID: falcon-h1-0.5b-instruct
   ğŸŒ Route: /falcon-h1-0.5b
   ğŸ“Š Mapped resources to ray_actor_options: {'num_cpus': 1.0, 'num_gpus': 0.45}
   âœ… Deployed successfully: falcon-h1-0.5b-instruct-app

================================================================================
âœ… Deployment Summary
================================================================================
Total applications deployed: 2/2

ğŸ“‹ Successfully Deployed Applications:
--------------------------------------------------------------------------------

â€¢ falcon3-1b-instruct-app
   Model: tiiuae/Falcon3-1B-Instruct
   Model ID: falcon3-1b-instruct
   Resources per replica: 1.0 CPUs, 0.45 GPUs
   Replicas: 1 â†’ 3 (autoscaling)
   Base URL: http://localhost:8000/falcon3-1b
   Endpoints:
     â€¢ POST /falcon3-1b/v1/chat/completions
     â€¢ POST /falcon3-1b/v1/completions
     â€¢ GET  /falcon3-1b/v1/models

â€¢ falcon-h1-0.5b-instruct-app
   Model: tiiuae/falcon-h1-0.5b-instruct
   Model ID: falcon-h1-0.5b-instruct
   Resources per replica: 1.0 CPUs, 0.45 GPUs
   Replicas: 1 â†’ 5 (autoscaling)
   Base URL: http://localhost:8000/falcon-h1-0.5b
   Endpoints:
     â€¢ POST /falcon-h1-0.5b/v1/chat/completions
     â€¢ POST /falcon-h1-0.5b/v1/completions
     â€¢ GET  /falcon-h1-0.5b/v1/models

================================================================================
ğŸ”¥ All deployments completed!
ğŸ’¡ Ray Dashboard: http://localhost:8265
ğŸ’¡ Check deployment status and resource usage in the dashboard
================================================================================
```

## ğŸ“ Config YAML

### Format hiá»‡n táº¡i (tá»± Ä‘á»™ng Ä‘Æ°á»£c xá»­ lÃ½):
```yaml
applications:
  - name: "falcon3-1b-instruct-app"
    import_path: "ray.serve.llm:build_openai_app"
    route_prefix: "/falcon3-1b"
    args:
      llm_configs:
        - model_loading_config:
            model_id: "falcon3-1b-instruct"
            model_source: "tiiuae/Falcon3-1B-Instruct"
          deployment_config:
            name: "falcon3-1b-deployment"
            ray_actor_options:
              num_cpus: 1        # â† Tá»± Ä‘á»™ng map vÃ o ray_actor_options
              num_gpus: 0.45     # â† Tá»± Ä‘á»™ng map vÃ o ray_actor_options
            autoscaling_config:
              min_replicas: 1
              max_replicas: 3
            # ... cÃ¡c config khÃ¡c
```

### Hoáº·c dÃ¹ng keys Ä‘Æ¡n giáº£n hÆ¡n:
```yaml
deployment_config:
  name: "my-deployment"
  num_cpus: 1      # â† app_v5.py sáº½ tá»± Ä‘á»™ng chuyá»ƒn thÃ nh ray_actor_options
  num_gpus: 0.45   # â† app_v5.py sáº½ tá»± Ä‘á»™ng chuyá»ƒn thÃ nh ray_actor_options
```

### Hoáº·c dÃ¹ng ray_actor_options trá»±c tiáº¿p (advanced):
```yaml
deployment_config:
  name: "my-deployment"
  ray_actor_options:
    num_cpus: 1
    num_gpus: 0.45
    resources:
      custom_resource: 1
    memory: 2000000000  # 2GB RAM
```

## ğŸ” Kiá»ƒm tra tÃ i nguyÃªn

### Trong Ray Dashboard (http://localhost:8265):
1. VÃ o tab **Serve**
2. Click vÃ o application name
3. Xem **Resources** cá»§a má»—i replica:
   - âœ… Má»—i replica cÃ³ Ä‘Ãºng 1 CPU + 0.45 GPU
   - âœ… KhÃ´ng cÃ³ replica nÃ o chiáº¿m full node

### Command line check:
```bash
# Xem status cá»§a cÃ¡c deployments
ray serve status

# Xem resource usage
ray status
```

## ğŸ“Š So sÃ¡nh V4 vs V5

| Feature | app_v4.py | app_v5.py |
|---------|-----------|-----------|
| **Deploy mode** | Sequential (tuáº§n tá»±) | Concurrent (Ä‘á»“ng thá»i) |
| **Speed** | Cháº­m vá»›i nhiá»u models | Nhanh hÆ¡n nhiá»u |
| **Resource per replica** | âŒ CÃ³ thá»ƒ láº¥y full node | âœ… ChÃ­nh xÃ¡c theo config |
| **Error handling** | Stop khi cÃ³ lá»—i | Continue vá»›i models khÃ¡c |
| **Thread pool** | KhÃ´ng | âœ… ThreadPoolExecutor |
| **Resource mapping** | Manual | âœ… Tá»± Ä‘á»™ng |

## ğŸ¯ VÃ­ dá»¥ thá»±c táº¿

### Scenario 1: Node cÃ³ 8 CPUs, 2 GPUs

**V4 behavior (cÃ³ thá»ƒ xáº£y ra):**
```
Model 1: Láº¥y 8 CPUs + 2 GPUs â†’ Model 2 khÃ´ng deploy Ä‘Æ°á»£c!
```

**V5 behavior (chÃ­nh xÃ¡c):**
```
Model 1: 1 CPU + 0.45 GPU (per replica)
Model 2: 1 CPU + 0.45 GPU (per replica)
â†’ CÃ³ thá»ƒ cháº¡y nhiá»u replicas trÃªn cÃ¹ng node!
```

### Scenario 2: Deploy 5 models cÃ¹ng lÃºc

**V4:**
```
Model 1 (3 phÃºt) â†’ Model 2 (3 phÃºt) â†’ Model 3 (3 phÃºt) â†’ Model 4 (3 phÃºt) â†’ Model 5 (3 phÃºt)
= Tá»•ng: 15 phÃºt
```

**V5:**
```
Model 1 (3 phÃºt) â”
Model 2 (3 phÃºt) â”œâ”€ Deploy song song
Model 3 (3 phÃºt) â”œâ”€ Deploy song song  
Model 4 (3 phÃºt) â”œâ”€ Deploy song song
Model 5 (3 phÃºt) â”˜
= Tá»•ng: ~3 phÃºt (náº¿u Ä‘á»§ resources)
```

## âš™ï¸ Advanced Options

### Giá»›i háº¡n sá»‘ threads deploy Ä‘á»“ng thá»i:
Máº·c Ä‘á»‹nh: `min(sá»‘_models, max(2, sá»‘_CPU_cores))`

Äá»ƒ thay Ä‘á»•i, edit trong `app_v5.py`:
```python
max_workers = 4  # Force 4 concurrent deployments
```

### ThÃªm custom resources:
```yaml
deployment_config:
  ray_actor_options:
    num_cpus: 1
    num_gpus: 0.45
    resources:
      my_custom_resource: 2
      accelerator_type:A100: 1
```

## ğŸ› Troubleshooting

### Lá»—i: "Insufficient resources"
**NguyÃªn nhÃ¢n:** Node khÃ´ng Ä‘á»§ tÃ i nguyÃªn cho táº¥t cáº£ replicas

**Giáº£i phÃ¡p:**
1. Giáº£m `min_replicas` trong config
2. Giáº£m `num_gpus` hoáº·c `num_cpus` per replica
3. ThÃªm nodes vÃ o Ray cluster

### Lá»—i: Model load lÃ¢u vÃ  timeout
**Giáº£i phÃ¡p:**
1. TÄƒng timeout trong Ray config
2. Pre-download models trÆ°á»›c khi deploy
3. DÃ¹ng local model path thay vÃ¬ download tá»« HuggingFace

### Má»™t sá»‘ models deploy failed
**V5 advantage:** KhÃ´ng áº£nh hÆ°á»Ÿng cÃ¡c models khÃ¡c!

Check logs Ä‘á»ƒ xem lá»—i cá»¥ thá»ƒ:
```bash
# Xem logs cá»§a deployment cá»¥ thá»ƒ
ray logs --follow deployment-name
```

## ğŸ“ˆ Best Practices

### 1. Resource Planning
```yaml
# VÃ­ dá»¥: Node cÃ³ 2 GPUs
# Muá»‘n cháº¡y 4 replicas â†’ Má»—i replica dÃ¹ng 0.5 GPU
deployment_config:
  ray_actor_options:
    num_gpus: 0.5
  autoscaling_config:
    min_replicas: 2
    max_replicas: 4
```

### 2. Autoscaling Config
```yaml
autoscaling_config:
  min_replicas: 1              # Start vá»›i 1 replica
  max_replicas: 5              # Scale up tá»‘i Ä‘a 5
  target_ongoing_requests: 85  # Scale khi cÃ³ >85 requests
```

### 3. Testing Strategy
```bash
# 1. Test vá»›i 1 model trÆ°á»›c
python3 app_v5.py test_single_model.yaml

# 2. Sau Ä‘Ã³ deploy táº¥t cáº£
python3 app_v5.py multi_model_config.yaml
```

## ğŸš€ Quick Start Checklist

- [ ] Ray Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t vÃ  start: `ray start --head`
- [ ] Config YAML Ä‘Ã£ sáºµn sÃ ng vá»›i resource specs
- [ ] Node cÃ³ Ä‘á»§ resources (check `ray status`)
- [ ] Run: `python3 app_v5.py multi_model_config.yaml`
- [ ] Check dashboard: http://localhost:8265
- [ ] Test endpoints vá»›i curl hoáº·c Postman

## ğŸ“š Additional Resources

- Ray Dashboard: http://localhost:8265
- Ray Serve Docs: https://docs.ray.io/en/latest/serve/
- Config examples: `multi_model_config.yaml`
- Old version: `app_v4.py` (for comparison)

## ğŸ‰ Summary

**app_v5.py = Concurrent + Per-Replica Resources**

ÄÃ¢y lÃ  version production-ready vá»›i:
- âœ… Deploy nhanh hÆ¡n (concurrent)
- âœ… Resource allocation chÃ­nh xÃ¡c (per-replica)
- âœ… Error handling robust
- âœ… Detailed logging vÃ  monitoring
- âœ… TÆ°Æ¡ng thÃ­ch vá»›i config hiá»‡n táº¡i

Enjoy! ğŸš€

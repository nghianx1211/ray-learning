# CPU_RESOURCE_ISSUE.md
# Gi·∫£i th√≠ch v·∫•n ƒë·ªÅ CPU Resource Contention

## üîç V·∫•n ƒë·ªÅ hi·ªán t·∫°i

M·∫∑c d√π b·∫°n ƒë√£ set `num_cpus: 0.5` cho m·ªói model replica, nh∆∞ng cluster v·∫´n h·∫øt 4/4 CPU. T·∫°i sao?

### Ph√¢n t√≠ch chi ti·∫øt:

```
M·ªói application t·∫°o ra 2 lo·∫°i deployments:
1. LLMServer (model server)  -> 0.5 CPU (ƒë√£ config)
2. LLMRouter (request router) -> 1 CPU √ó 2 replicas = 2 CPU (m·∫∑c ƒë·ªãnh c·ªßa Ray Serve)

App 1 (falcon3-1b):
  - LLMServer: 0.5 CPU
  - LLMRouter: 2 √ó 1 CPU = 2 CPU
  - T·ªïng: 2.5 CPU

App 2 (falcon-h1-0.5b):
  - LLMServer: 0.5 CPU
  - LLMRouter: 2 √ó 1 CPU = 2 CPU
  - T·ªïng: 2.5 CPU

T·ªîNG C·ªòNG: 5 CPU
Available: 4 CPU
‚ùå Thi·∫øu: 1 CPU
```

### Log t·ª´ `ray status`:
```
Total Usage:
 4.0/4.0 CPU (2.0 used of 2.0 reserved in placement groups)
 0.8/1.0 GPU (0.8 used of 0.8 reserved in placement groups)

Total Demands:
 {'CPU': 1.0}: 2+ pending tasks/actors  # <- 2 LLMRouter replicas ƒëang ch·ªù CPU
```

## ‚úÖ Gi·∫£i ph√°p

### Option 1: Deploy 1 model duy nh·∫•t (KHUY·∫æN NGH·ªä cho single-GPU node)

```bash
python3 app_v5.py single_model_optimized.yaml
```

**L·ª£i √≠ch:**
- ƒê·ªß resources cho 1 model + LLMRouter
- T·ªëi ∆∞u h√≥a GPU utilization (0.9)
- TƒÉng throughput v·ªõi batch size l·ªõn h∆°n
- Kh√¥ng b·ªã resource contention

**Resource allocation:**
```
LLMServer: 1 CPU + 0.9 GPU
LLMRouter: 2 CPU (2 replicas √ó 1 CPU)
T·ªïng: 3 CPU + 0.9 GPU
C√≤n d∆∞: 1 CPU cho Ray system ‚úÖ
```

### Option 2: Deploy 2 models v·ªõi resources gi·∫£m (C√≥ th·ªÉ g·∫∑p performance issues)

File hi·ªán t·∫°i: `multi_model_config.yaml` ƒë√£ ƒë∆∞·ª£c update v·ªõi:
- `max_replicas: 1` (gi·∫£m t·ª´ 2-3)
- `router_config` th√™m ƒë·ªÉ gi·ªõi h·∫°n LLMRouter

**L∆∞u √Ω:** `router_config` c√≥ th·ªÉ kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£ b·ªüi `build_openai_app`. 
N·∫øu v·∫´n g·∫∑p l·ªói, LLMRouter s·∫Ω v·∫´n s·ª≠ d·ª•ng 2 replicas √ó 1 CPU.

### Option 3: TƒÉng s·ªë node ho·∫∑c CPU/GPU

```bash
# Th√™m worker node
ray start --address='<head-node-ip>:6379' --num-cpus=4 --num-gpus=1

# Ho·∫∑c s·ª≠ d·ª•ng cloud autoscaling
```

## üìù Testing

### 1. Test v·ªõi 1 model:
```bash
# Shutdown deployment hi·ªán t·∫°i
serve shutdown -y

# Deploy 1 model
python3 app_v5.py single_model_optimized.yaml

# Ki·ªÉm tra status
serve status
ray status

# Test endpoint
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "falcon3-1b-instruct",
    "messages": [{"role": "user", "content": "Hello!"}],
    "temperature": 0.7
  }'
```

### 2. Test v·ªõi 2 models (n·∫øu mu·ªën th·ª≠):
```bash
serve shutdown -y
python3 app_v5.py multi_model_config.yaml
```

**D·ª± ƒëo√°n:** V·∫´n s·∫Ω g·∫∑p l·ªói CPU contention do LLMRouter m·∫∑c ƒë·ªãnh 2 replicas.

## üéØ Khuy·∫øn ngh·ªã cu·ªëi c√πng

V·ªõi single-GPU node (4 CPU, 1 GPU), **n√™n deploy 1 model duy nh·∫•t** ƒë·ªÉ:
1. Tr√°nh resource contention
2. T·ªëi ∆∞u h√≥a performance
3. D·ªÖ d√†ng monitor v√† debug

N·∫øu c·∫ßn deploy nhi·ªÅu models:
- Scale horizontally: Th√™m worker nodes
- Scale vertically: Upgrade node specs (more CPUs)
- Ho·∫∑c deploy models theo ki·ªÉu "on-demand" (load/unload theo request)

## üìö References

- Ray Serve docs: https://docs.ray.io/en/latest/serve/
- Resource allocation: https://docs.ray.io/en/latest/ray-core/scheduling/resources.html
- Placement groups: https://docs.ray.io/en/latest/ray-core/scheduling/placement-group.html

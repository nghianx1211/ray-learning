applications:
  # Application 1: Falcon 3-1B Instruct
  - name: "falcon3-1b-instruct-app"
    import_path: "ray.serve.llm:build_openai_app"
    route_prefix: "/falcon3-1b"
    args:
      llm_configs:
        - model_loading_config:
            model_id: "falcon3-1b-instruct"
            model_source: "tiiuae/Falcon3-1B-Instruct"
            type: "VLLM"
          deployment_config:
            name: "falcon3-1b-deployment"
            ray_actor_options:
              num_cpus: 1  # Tăng CPU để handle concurrency tốt hơn
              num_gpus: 0.4  # Sử dụng gần như full GPU
            autoscaling_config:
              min_replicas: 1
              max_replicas: 1  # Single GPU = single replica
              target_ongoing_requests: 50
            max_ongoing_requests: 256  # Tăng concurrency limit
          engine_kwargs:
            tensor_parallel_size: 0  # KHÔNG dùng TP trên single GPU (yêu cầu 2+ GPUs)
            dtype: "float16"
            gpu_memory_utilization: 0.4  # Giảm để chia sẻ GPU
            max_model_len: 4096  # Giảm xuống để tiết kiệm memory
            enforce_eager: false  # Sử dụng CUDA graphs để tăng tốc
            max_num_seqs: 128  # Giảm sequences
            max_num_batched_tokens: 4096  # Giảm batch size
            trust_remote_code: true
            disable_custom_all_reduce: true

  - name: "falcon3-3b-instruct-app"
    import_path: "ray.serve.llm:build_openai_app"
    route_prefix: "/falcon3-3b"
    args:
      llm_configs:
        - model_loading_config:
            model_id: "falcon3-3b-instruct"
            model_source: "tiiuae/Falcon3-3B-Instruct"
            type: "VLLM"
          deployment_config:
            name: "falcon3-3b-deployment"
            ray_actor_options:
              num_cpus: 1  # Tăng CPU để handle concurrency tốt hơn
              num_gpus: 0.4  # Sử dụng gần như full GPU
            autoscaling_config:
              min_replicas: 1
              max_replicas: 1  # Single GPU = single replica
              target_ongoing_requests: 50
            max_ongoing_requests: 256  # Tăng concurrency limit
          engine_kwargs:
            tensor_parallel_size: 0  # KHÔNG dùng TP trên single GPU (yêu cầu 2+ GPUs)
            dtype: "float16"
            gpu_memory_utilization: 0.55  # Giảm để chia sẻ GPU
            max_model_len: 4096  # Giảm xuống để tiết kiệm memory
            enforce_eager: false  # Sử dụng CUDA graphs để tăng tốc
            max_num_seqs: 128  # Giảm sequences
            max_num_batched_tokens: 4096  # Giảm batch size
            trust_remote_code: true
            disable_custom_all_reduce: true

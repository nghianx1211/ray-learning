# ============================================================
# Core Ray Serve Requirements
# ============================================================
# Note: vLLM 0.10+ requires Ray >= 2.43.0
# Using Ray 2.49.0 for latest stable version and vLLM 0.10+ compatibility
ray[serve]==2.49.0

# ============================================================
# VLLM for LLM Serving
# ============================================================
vllm>=0.10.0,<0.11.0
# Note: vllm already includes: torch, transformers, tokenizers

# ============================================================
# Model & Training Utils (compatible versions)
# ============================================================
# Note: vLLM bundles torch, transformers, tokenizers
# Only specify if you need specific versions
transformers>=4.30.0,<5.0.0
tokenizers>=0.13.0,<1.0.0

# ============================================================
# HTTP & API (compatible with vLLM 0.10+)
# ============================================================
fastapi>=0.115.0
uvicorn>=0.22.0
pydantic>=2.0.0,<3.0.0
async-timeout>=4.0.0,<5.0.0

# ============================================================
# Monitoring & Logging
# ============================================================
rich>=13.0.0,<14.0.0
pydantic-settings>=2.0.0,<3.0.0

# ============================================================
# Cloud Storage (optional - comment out if not needed)
# ============================================================
google-cloud-storage>=2.10.0,<3.0.0
boto3>=1.28.0,<2.0.0

# ============================================================
# YAML & Config
# ============================================================
pyyaml>=6.0.0,<7.0.0

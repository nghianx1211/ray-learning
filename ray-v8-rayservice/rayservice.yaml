# ============================================================
# RayService Configuration for Multi-Model LLM Deployment
# Ray version: 2.49.0 | VLLM version: 0.10+
# ============================================================
apiVersion: ray.io/v1
kind: RayService
metadata:
  name: multi-model-llm-service
  namespace: default
  labels:
    app: ray-vllm
    version: v8
    environment: production
  annotations:
    description: "Multi-model LLM serving with Ray Serve and VLLM"
    maintainer: "AI Team"
spec:
  # ============================================================
  # Service Configuration
  # ============================================================
  serviceUnhealthySecondThreshold: 900  # 15 minutes
  deploymentUnhealthySecondThreshold: 300  # 5 minutes
  
  # ============================================================
  # Serve Configuration V2
  # Using builders.rayservice_wrapper for compatibility
  # ============================================================
  serveConfigV2: |
    applications:
      - name: falcone-3b-instruct
        import_path: builders.rayservice_wrapper:build_app_from_args
        route_prefix: /
        runtime_env:
          working_dir: "."
          env_vars:
            MODEL_CONFIG_PATH: "/config/model_config.yaml"
            CUDA_VISIBLE_DEVICES: "0"
            RAY_TMPDIR: "/tmp/ray"
            TRANSFORMERS_CACHE: "/mnt/models/.cache"
            HF_HOME: "/mnt/models/.cache"
            PYTHONUNBUFFERED: "1"
            VLLM_USE_V1: "0"
        args:
          config_path: "/config/model_config.yaml"
  
  # ============================================================
  # Ray Cluster Configuration
  # ============================================================
  rayClusterConfig:
    rayVersion: "2.49.0"
    enableInTreeAutoscaling: true
    
    # ============================================================
    # Head Node Configuration
    # ============================================================
    headGroupSpec:
      serviceType: ClusterIP
      rayStartParams:
        dashboard-host: "0.0.0.0"
        port: "6379"
        num-cpus: "0"  # Head node doesn't process tasks
        object-store-memory: "2000000000"  # 2GB object store
        metrics-export-port: "8080"
      
      template:
        metadata:
          labels:
            component: ray-head
            app: ray-vllm
        spec:
          # Security context
          securityContext:
            runAsUser: 1000
            runAsGroup: 1000
            fsGroup: 1000
          
          # Init container to set up cache directory
          initContainers:
            - name: init-cache
              image: busybox:1.35
              command: ['sh', '-c']
              args:
                - |
                  mkdir -p /mnt/models/.cache
                  chmod 755 /mnt/models/.cache
              volumeMounts:
                - name: model-storage
                  mountPath: /mnt/models
          
          containers:
            - name: ray-head
              image: asia-southeast1-docker.pkg.dev/kubernetes-468114/test/ray-vllm-serve:v8-latest
              imagePullPolicy: Always
              
              # Resource requests and limits
              resources:
                requests:
                  cpu: "2"
                  memory: "4Gi"
                  ephemeral-storage: "10Gi"
                limits:
                  cpu: "4"
                  memory: "8Gi"
                  ephemeral-storage: "20Gi"
              
              # Environment variables
              env:
                - name: RAY_ENABLE_AUTO_CONNECT
                  value: "0"
                - name: RAY_TMPDIR
                  value: "/tmp/ray"
                - name: PYTHONUNBUFFERED
                  value: "1"
              
              # Volume mounts
              volumeMounts:
                - name: model-config
                  mountPath: /config
                  readOnly: true
                - name: model-storage
                  mountPath: /mnt/models
                - name: tmp
                  mountPath: /tmp
              
              # Ports
              ports:
                - containerPort: 6379  # Redis port
                  name: redis
                  protocol: TCP
                - containerPort: 8265  # Dashboard
                  name: dashboard
                  protocol: TCP
                - containerPort: 8000  # Serve HTTP
                  name: serve
                  protocol: TCP
                - containerPort: 8080  # Metrics
                  name: metrics
                  protocol: TCP
              
              # Liveness probe
              livenessProbe:
                httpGet:
                  path: /
                  port: 8265
                initialDelaySeconds: 30
                periodSeconds: 10
                timeoutSeconds: 5
                failureThreshold: 3
              
              # Readiness probe
              readinessProbe:
                httpGet:
                  path: /
                  port: 8265
                initialDelaySeconds: 20
                periodSeconds: 5
                timeoutSeconds: 3
                failureThreshold: 3
          
          # Volumes
          volumes:
            - name: model-config
              configMap:
                name: ray-model-config
            - name: model-storage
              persistentVolumeClaim:
                claimName: ray-model-storage
            - name: tmp
              emptyDir: {}
    
    # ============================================================
    # Worker Group Specifications
    # ============================================================
    workerGroupSpecs:
      # GPU Workers for LLM Inference
      - groupName: gpu-workers
        replicas: 1
        minReplicas: 1
        maxReplicas: 4
        
        rayStartParams:
          num-cpus: "4"
          num-gpus: "1"
          object-store-memory: "4000000000"  # 4GB object store
          metrics-export-port: "8080"
        
        template:
          metadata:
            labels:
              component: ray-worker
              app: ray-vllm
              workload: gpu
          spec:
            # Security context
            securityContext:
              runAsUser: 1000
              runAsGroup: 1000
              fsGroup: 1000
            
            # Node selector for GPU nodes
            nodeSelector:
              node-type: "gpu"
              accelerator: "nvidia-tesla-t4"  # Adjust based on your GPU type
            
            # Tolerations for GPU nodes
            tolerations:
              - key: "nvidia.com/gpu"
                operator: "Exists"
                effect: "NoSchedule"
              - key: "workload"
                operator: "Equal"
                value: "training"
                effect: "NoSchedule"
            
            # Affinity rules
            affinity:
              # Prefer to spread workers across nodes
              podAntiAffinity:
                preferredDuringSchedulingIgnoredDuringExecution:
                  - weight: 100
                    podAffinityTerm:
                      labelSelector:
                        matchExpressions:
                          - key: component
                            operator: In
                            values:
                              - ray-worker
                      topologyKey: kubernetes.io/hostname
            
            # Init container
            initContainers:
              - name: init-cache
                image: busybox:1.35
                command: ['sh', '-c']
                args:
                  - |
                    mkdir -p /mnt/models/.cache
                    chmod 755 /mnt/models/.cache
                volumeMounts:
                  - name: model-storage
                    mountPath: /mnt/models
            
            containers:
              - name: ray-worker
                image: asia-southeast1-docker.pkg.dev/kubernetes-468114/test/ray-vllm-serve:v8-latest
                imagePullPolicy: Always
                
                # Resource requests and limits
                resources:
                  requests:
                    cpu: "4"
                    memory: "16Gi"
                    nvidia.com/gpu: "1"
                    ephemeral-storage: "20Gi"
                  limits:
                    cpu: "8"
                    memory: "32Gi"
                    nvidia.com/gpu: "1"
                    ephemeral-storage: "40Gi"
                
                # Environment variables
                env:
                  - name: RAY_ENABLE_AUTO_CONNECT
                    value: "0"
                  - name: CUDA_VISIBLE_DEVICES
                    value: "0"
                  - name: RAY_TMPDIR
                    value: "/tmp/ray"
                  - name: TRANSFORMERS_CACHE
                    value: "/mnt/models/.cache"
                  - name: HF_HOME
                    value: "/mnt/models/.cache"
                  - name: PYTHONUNBUFFERED
                    value: "1"
                  - name: VLLM_USE_V1
                    value: "0"
                
                # Volume mounts
                volumeMounts:
                  - name: model-config
                    mountPath: /config
                    readOnly: true
                  - name: model-storage
                    mountPath: /mnt/models
                  - name: tmp
                    mountPath: /tmp
                  - name: shm
                    mountPath: /dev/shm
                
                # Ports
                ports:
                  - containerPort: 8080
                    name: metrics
                    protocol: TCP
                
                # Liveness probe
                livenessProbe:
                  exec:
                    command:
                      - sh
                      - -c
                      - |
                        ray status
                  initialDelaySeconds: 60
                  periodSeconds: 30
                  timeoutSeconds: 10
                  failureThreshold: 3
                
                # Readiness probe
                readinessProbe:
                  exec:
                    command:
                      - sh
                      - -c
                      - |
                        ray status
                  initialDelaySeconds: 30
                  periodSeconds: 10
                  timeoutSeconds: 5
                  failureThreshold: 3
            
            # Volumes
            volumes:
              - name: model-config
                configMap:
                  name: ray-model-config
              - name: model-storage
                persistentVolumeClaim:
                  claimName: ray-model-storage
              - name: tmp
                emptyDir: {}
              - name: shm
                emptyDir:
                  medium: Memory
                  sizeLimit: "8Gi"  # Shared memory for CUDA

---
# ============================================================
# ConfigMap for Model Configuration
# ============================================================
apiVersion: v1
kind: ConfigMap
metadata:
  name: ray-model-config
  namespace: default
  labels:
    app: ray-vllm
data:
  model_config.yaml: |
    applications:
      # ============================================================
      # Falcon-E 3B Instruct Model
      # ============================================================
      - name: "falcone-3b-instruct-app"
        route_prefix: "/falcone-3b-instruct"
        args:
          llm_configs:
            - model_loading_config:
                model_id: "falcone-3b-instruct"
                # Use local cache path or GCS bucket
                model_source: "/mnt/models/.cache/falcon-models/Falcon-E-3B-Instruct-bfloat16"
                # Alternative: Use GCS bucket if uploaded
                # model_source: "gs://falcon-models/Falcon-E-3B-Instruct-bfloat16"
                type: "VLLM"
              
              deployment_config:
                name: "falcone-3b-instruct-deployment"
                ray_actor_options:
                  num_cpus: 2
                  num_gpus: 0.6
                autoscaling_config:
                  min_replicas: 1
                  max_replicas: 2
                  target_ongoing_requests: 50
                max_ongoing_requests: 256
              
              engine_kwargs:
                tensor_parallel_size: 0
                revision: "bfloat16"
                dtype: "bfloat16"
                gpu_memory_utilization: 0.6
                max_model_len: 4096
                enforce_eager: false
                max_num_seqs: 128
                max_num_batched_tokens: 4096
                trust_remote_code: true
                disable_custom_all_reduce: true
      
      # ============================================================
      # Falcon3 1B Instruct Model (Uncomment to enable)
      # ============================================================
      # - name: "falcon3-1b-instruct-app"
      #   route_prefix: "/falcon3-1b"
      #   args:
      #     llm_configs:
      #       - model_loading_config:
      #           model_id: "falcon3-1b-instruct"
      #           model_source: "tiiuae/Falcon3-1B-Instruct"
      #           type: "VLLM"
      #         
      #         deployment_config:
      #           name: "falcon3-1b-deployment"
      #           ray_actor_options:
      #             num_cpus: 1
      #             num_gpus: 0.4
      #           autoscaling_config:
      #             min_replicas: 1
      #             max_replicas: 2
      #             target_ongoing_requests: 50
      #           max_ongoing_requests: 256
      #         
      #         engine_kwargs:
      #           tensor_parallel_size: 0
      #           dtype: "bfloat16"
      #           gpu_memory_utilization: 0.35
      #           max_model_len: 4096
      #           enforce_eager: false
      #           max_num_seqs: 128
      #           max_num_batched_tokens: 4096
      #           trust_remote_code: true
      #           disable_custom_all_reduce: true

---
# ============================================================
# PersistentVolumeClaim for Model Storage
# ============================================================
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ray-model-storage
  namespace: default
  labels:
    app: ray-vllm
spec:
  accessModes:
    - ReadWriteMany  # Allow multiple pods to access
  resources:
    requests:
      storage: 100Gi  # Adjust based on model sizes
  storageClassName: standard-rwx  # Adjust to your storage class

---
# ============================================================
# Service for External Access
# ============================================================
apiVersion: v1
kind: Service
metadata:
  name: ray-vllm-serve
  namespace: default
  labels:
    app: ray-vllm
spec:
  type: LoadBalancer  # Change to ClusterIP if using Ingress
  selector:
    component: ray-head
    app: ray-vllm
  ports:
    - name: serve
      port: 8000
      targetPort: 8000
      protocol: TCP
    - name: dashboard
      port: 8265
      targetPort: 8265
      protocol: TCP

---
# ============================================================
# Ingress for HTTPS Access (Optional)
# ============================================================
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ray-vllm-ingress
  namespace: default
  labels:
    app: ray-vllm
  annotations:
    kubernetes.io/ingress.class: "nginx"
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    nginx.ingress.kubernetes.io/proxy-body-size: "100m"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "600"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "600"
spec:
  tls:
    - hosts:
        - ray-llm.yourdomain.com  # Replace with your domain
      secretName: ray-llm-tls
  rules:
    - host: ray-llm.yourdomain.com  # Replace with your domain
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: ray-vllm-serve
                port:
                  number: 8000

---
# ============================================================
# HorizontalPodAutoscaler (Optional)
# ============================================================
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ray-worker-hpa
  namespace: default
  labels:
    app: ray-vllm
spec:
  scaleTargetRef:
    apiVersion: ray.io/v1
    kind: RayCluster
    name: multi-model-llm-service
  minReplicas: 1
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
        - type: Percent
          value: 50
          periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Percent
          value: 25
          periodSeconds: 120

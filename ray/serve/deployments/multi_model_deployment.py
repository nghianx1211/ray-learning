# deployments/multimodel_deployment.py
from ray import serve
import asyncio
import os
from typing import AsyncGenerator, Any, Dict, List
from ray.llm._internal.serve.configs.server_models import LLMConfig
from ray.llm._internal.serve.observability.logging import get_logger

from serve.deployments.multi_model_server import MultiModelServer

logger = get_logger(__name__)


@serve.deployment(
    autoscaling_config={
        "min_replicas": 1,
        "initial_replicas": 1,
        "max_replicas": 2,
        "target_ongoing_requests": 50,
    },
    max_ongoing_requests=256,
    health_check_period_s=10,
    health_check_timeout_s=5,
)
class MultiModelDeployment:
    """
    Deployment host nhi·ªÅu lo·∫°i model (VLLM, AUDIO, VISION, VIDEO) trong c√πng server.
    M·ªói model s·∫Ω c√≥ metadata ri√™ng, router s·∫Ω d√πng get_config() ƒë·ªÉ ƒëƒÉng k√Ω model.
    """

    def __init__(self, llm_configs: List[LLMConfig]):
        # CRITICAL: Ensure CUDA_VISIBLE_DEVICES is set in actual OS environment
        # This is required for VLLM multiprocessing.spawn to work properly
        cuda_devices = os.environ.get('CUDA_VISIBLE_DEVICES', '')
        if not cuda_devices:
            # If not set, default to GPU 0
            cuda_devices = '0'
            os.environ['CUDA_VISIBLE_DEVICES'] = cuda_devices
        
        logger.info(f"‚öôÔ∏è CUDA_VISIBLE_DEVICES={cuda_devices}")
        
        # Server qu·∫£n l√Ω nhi·ªÅu model kh√°c nhau
        self.server = MultiModelServer(llm_configs)
        self.models: Dict[str, Dict[str, Any]] = {}

        for cfg in llm_configs:
            model_id = cfg.model_loading_config.get("model_id", "unknown-model")
            model_type = cfg.model_loading_config.get("type", "VLLM").upper()
            self.models[model_id] = {
                "model_id": model_id,
                "type": model_type,
                "engine_kwargs": cfg.engine_kwargs,
                "deployment_config": cfg.deployment_config,
            }

            logger.info(
                f"üß© Registered model in deployment: {model_id} ({model_type})"
            )

        # Kh·ªüi ƒë·ªông async server - ƒë·ª£i cho ƒë·∫øn khi ho√†n t·∫•t
        self._start_task = None

    async def _ensure_started(self):
        """Ensure server is started before processing requests."""
        if self._start_task is None:
            self._start_task = asyncio.create_task(self.server.start())
        await self._start_task

    # ------------------------------------------------------------------
    # Public API (Router s·∫Ω g·ªçi qua DeploymentHandle)
    # ------------------------------------------------------------------
    async def get_config(self) -> Dict[str, Any]:
        """
        Return metadata cho router bi·∫øt model_id v√† type.
        Router s·∫Ω t·ª± ƒë·ªông g·ªçi h√†m n√†y khi ƒëƒÉng k√Ω model.
        """
        return {
            "models": list(self.models.values()),
            "deployment_name": self.__class__.__name__,
        }

    async def check_health(self) -> Dict[str, Any]:
        """Ki·ªÉm tra health c·ªßa t·∫•t c·∫£ model."""
        await self._ensure_started()
        status = await self.server.check_health()
        return {"status": "healthy", "models": list(self.models.keys())}

    async def infer(self, request: Dict[str, Any]) -> AsyncGenerator[Any, None]:
        """
        Unified inference entrypoint ‚Äî d√πng cho m·ªçi lo·∫°i model.
        Expect body: {"model_id": "...", "input": "...", "params": {...}}
        """
        logger.info(f"Received inference request: {request}")

        model_id = request.get("model_id")
        if not model_id:
            logger.warning("Missing model_id in request")
            yield {"error": "Missing model_id in request"}
            return

        if model_id not in self.models:
            logger.warning(f"Model {model_id} not found in deployment")
            yield {"error": f"Model {model_id} not found in deployment"}
            return

        logger.info(f"üîÅ Running inference for model {model_id}")
        
        # Ensure server is started
        await self._ensure_started()
        logger.info(f"Server is ready, dispatching inference...")

        try:
            async for chunk in self.server.infer(model_id, request):
                logger.debug(f"Yielding chunk for model {model_id}: {chunk}")
                yield chunk
            logger.info(f"Inference completed for model {model_id}")
        except Exception as e:
            logger.error(f"‚ùå Inference failed for {model_id}: {e}")
            yield {"error": str(e)}
